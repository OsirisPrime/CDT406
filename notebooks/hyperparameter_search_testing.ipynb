{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "#from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Pre-processing functions\n",
    "from src.data.data_helper import get_raw_data_as_dataframe\n",
    "from src.models.preprocessing.preprocessor import SignalPreprocessor\n",
    "from src.data.data_helper import segement_data\n",
    "\n",
    "# Model functions\n",
    "from src.models.LSTM.LSTM import LSTM\n",
    "from src.models.LSTM_STFT.LSTM_STFT import LSTM_STFT\n",
    "from src.models.LSTM_STFT_Dense.LSTM_STFT_Dense import LSTM_STFT_Dense"
   ],
   "id": "2f50b9784804cb83"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_training_data():\n",
    "    # Bandpass filter parameters\n",
    "    bandpass_order = 7\n",
    "    high_freq = 500.0\n",
    "    low_freq = 20.0\n",
    "    fs = 5000.0\n",
    "\n",
    "    raw_data = get_raw_data_as_dataframe()\n",
    "\n",
    "    # Initialize the preprocessor\n",
    "    pre_processor = SignalPreprocessor(low_freq=low_freq, high_freq=high_freq, fs=fs, order=bandpass_order)\n",
    "    # Calibrate the preprocessor\n",
    "    pre_processor.calibrate(raw_data)\n",
    "\n",
    "    segmented_data = segement_data(raw_data, window_length=200 * 5, overlap=50 * 5)\n",
    "    num_classes = segmented_data['label'].nunique()\n",
    "\n",
    "    y_data = np.array(segmented_data['label'].values)\n",
    "    y_data = tf.keras.utils.to_categorical(y_data, num_classes=num_classes)\n",
    "\n",
    "    X_data = np.stack(segmented_data.drop(columns=['label', 'source'])['window_data'].values)\n",
    "    X_data = pre_processor.batch_pre_process(X_data)\n",
    "\n",
    "    input_shape = X_data.shape[1]\n",
    "\n",
    "    return X_data, y_data, num_classes, input_shape"
   ],
   "id": "8087ac94549444f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "X_data, y_data, num_classes, input_shape = get_training_data()",
   "id": "677c0392e9c071a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_data, y_data, test_size=0.2\n",
    "    )"
   ],
   "id": "77281b915381121b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "import keras_tuner as kt",
   "id": "c00bf462522e2cf3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class LSTMHyperModel(kt.HyperModel):\n",
    "\n",
    "    def __init__(self, input_shape, num_classes):\n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    # ----------------- build -----------------------------------------\n",
    "    def build(self, hp):\n",
    "        lr   = hp.Choice('learning_rate',\n",
    "                         values=[1e-1, 2e-2, 5e-2, 1e-3, 2e-3, 5e-3])\n",
    "\n",
    "        opt  = hp.Choice('optimizer',\n",
    "                         values=['adam', 'rmsprop', 'nadam'])\n",
    "\n",
    "        norm = hp.Choice('normalization',\n",
    "                         values=['none', 'batch', 'layer'])\n",
    "\n",
    "        # We only declare batch_size here; we'll use it in fit().\n",
    "        hp.Choice('batch_size', values=[32, 64, 128, 256, 512])\n",
    "\n",
    "        model = LSTM(self.input_shape,\n",
    "                     self.num_classes,\n",
    "                     learning_rate=lr,\n",
    "                     optimizer=opt,\n",
    "                     normalization=norm).get_model()\n",
    "        return model\n",
    "\n",
    "    # ----------------- fit -------------------------------------------\n",
    "    def fit(self, hp, model, X_train, y_train, X_val, y_val, **kwargs):\n",
    "        \"\"\"\n",
    "        Called by the tuner for every trial.  We inject the per-trial\n",
    "        batch_size coming from hp.\n",
    "        \"\"\"\n",
    "        batch_size = hp.get('batch_size')\n",
    "        return model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            batch_size=batch_size,\n",
    "            epochs=kwargs.get('epochs', 10),\n",
    "            verbose=kwargs.get('verbose', 2)\n",
    "        )"
   ],
   "id": "6d8edc248560fdcf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "stop_early = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_f1_score',      # metric name Keras assigns: ‘f1_score’\n",
    "    mode='max',                  # we want to maximise it\n",
    "    patience=5,\n",
    "    restore_best_weights=True)"
   ],
   "id": "bff581170f7d9e96"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "hypermodel = LSTMHyperModel(input_shape, num_classes)",
   "id": "58ec55430db62dc4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from src.utils.path_utils import get_models_dir\n",
    "\n",
    "model_dir = get_models_dir() / \"LSTM_search\"\n",
    "model_dir"
   ],
   "id": "98f2f625c1de8688"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "tuner = kt.BayesianOptimization(\n",
    "    hypermodel,\n",
    "    objective = kt.Objective(\"val_f1_score\", direction=\"max\"),\n",
    "    max_trials=5,\n",
    "    directory=model_dir,\n",
    "    project_name=\"baseline_v2\",\n",
    "    overwrite=True\n",
    ")"
   ],
   "id": "a94b0235d7927ede"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "tuner.search(X_train, y_train,\n",
    "             X_val=X_val, y_val=y_val,\n",
    "             callbacks=[stop_early],\n",
    "             epochs=30,\n",
    "             verbose=1)"
   ],
   "id": "a746307b6721a6a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]"
   ],
   "id": "5a6209d17c2c3452"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Retrieve the best trial’s hyper-parameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete.\n",
    "\n",
    "Optimal learning rate : {best_hps.get('learning_rate')}\n",
    "Optimal optimizer      : {best_hps.get('optimizer')}\n",
    "Optimal normalization  : {best_hps.get('normalization')}\n",
    "Optimal batch size     : {best_hps.get('batch_size')}\n",
    "\"\"\")"
   ],
   "id": "fdb26e1eb3e10bcf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "46663a832061d6f"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
