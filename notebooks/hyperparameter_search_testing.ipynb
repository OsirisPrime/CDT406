{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T14:51:56.581802Z",
     "start_time": "2025-05-13T14:51:56.474888Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "# Pre-processing functions\n",
    "from src.data.data_helper import get_raw_data_as_dataframe\n",
    "from src.models.preprocessing.preprocessor import SignalPreprocessor\n",
    "from src.data.data_helper import segement_data\n",
    "\n",
    "# Model functions\n",
    "from src.models.LSTM.LSTM import LSTM\n",
    "from src.models.LSTM_STFT.LSTM_STFT import LSTM_STFT\n",
    "from src.models.LSTM_STFT_Dense.LSTM_STFT_Dense import LSTM_STFT_Dense"
   ],
   "id": "f41472cd74952d54",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T14:51:56.728709Z",
     "start_time": "2025-05-13T14:51:56.690336Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_training_data():\n",
    "    raw_train, raw_val = get_raw_data_as_dataframe(\n",
    "        validation_subjects=(1,2)\n",
    "    )\n",
    "\n",
    "    # Initialize the preprocessor\n",
    "    pre_processor = SignalPreprocessor(\n",
    "        low_freq=20.0,\n",
    "        high_freq=500.0,\n",
    "        fs=5000.0,\n",
    "        order=7\n",
    "    )\n",
    "    # Calibrate the preprocessor\n",
    "    pre_processor.calibrate(raw_train)\n",
    "\n",
    "    window_length=200 * 5\n",
    "    overlap=50 * 5\n",
    "\n",
    "    seg_train = segement_data(\n",
    "        raw_train, window_length=window_length, overlap=overlap\n",
    "    )\n",
    "    seg_val = segement_data(\n",
    "        raw_val,   window_length=window_length, overlap=overlap\n",
    "    )\n",
    "\n",
    "    all_labels = pd.concat([seg_train['label'], seg_val['label']])\n",
    "    num_classes = all_labels.nunique()\n",
    "\n",
    "    y_train = tf.keras.utils.to_categorical(\n",
    "        seg_train['label'].values, num_classes=num_classes\n",
    "    )\n",
    "    y_val = tf.keras.utils.to_categorical(\n",
    "        seg_val['label'].values,  num_classes=num_classes\n",
    "    )\n",
    "\n",
    "    X_train = np.stack(seg_train.drop(columns=['label', 'source'])['window_data'].values)\n",
    "    X_val   = np.stack(seg_val.drop(columns=['label', 'source'])['window_data'].values)\n",
    "\n",
    "    X_train = pre_processor.batch_pre_process(X_train)\n",
    "    X_val   = pre_processor.batch_pre_process(X_val)\n",
    "\n",
    "    input_shape = X_train.shape[1]\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, num_classes, input_shape"
   ],
   "id": "305ae0daa69fab3b",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-05-13T14:51:56.744929Z"
    }
   },
   "cell_type": "code",
   "source": "X_train, y_train, X_val, y_val, num_classes, input_shape = get_training_data()",
   "id": "19c8ec7c01c59989",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[29]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m X_train, y_train, X_val, y_val, num_classes, input_shape = get_training_data()\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[28]\u001B[39m\u001B[32m, line 14\u001B[39m, in \u001B[36mget_training_data\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m      7\u001B[39m pre_processor = SignalPreprocessor(\n\u001B[32m      8\u001B[39m     low_freq=\u001B[32m20.0\u001B[39m,\n\u001B[32m      9\u001B[39m     high_freq=\u001B[32m500.0\u001B[39m,\n\u001B[32m     10\u001B[39m     fs=\u001B[32m5000.0\u001B[39m,\n\u001B[32m     11\u001B[39m     order=\u001B[32m7\u001B[39m\n\u001B[32m     12\u001B[39m )\n\u001B[32m     13\u001B[39m \u001B[38;5;66;03m# Calibrate the preprocessor\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m14\u001B[39m pre_processor.calibrate(raw_train)\n\u001B[32m     16\u001B[39m window_length=\u001B[32m200\u001B[39m * \u001B[32m5\u001B[39m\n\u001B[32m     17\u001B[39m overlap=\u001B[32m50\u001B[39m * \u001B[32m5\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\IES_codebase\\EMG_Project\\CDT406-Smart-Gripper\\src\\models\\preprocessing\\preprocessor.py:28\u001B[39m, in \u001B[36mcalibrate\u001B[39m\u001B[34m(self, raw_data)\u001B[39m\n\u001B[32m     27\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcalibrate\u001B[39m(\u001B[38;5;28mself\u001B[39m, raw_data):\n\u001B[32m---> \u001B[39m\u001B[32m28\u001B[39m     processed_signals = []\n\u001B[32m     29\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m source, group \u001B[38;5;129;01min\u001B[39;00m raw_data.groupby(\u001B[33m'\u001B[39m\u001B[33msource\u001B[39m\u001B[33m'\u001B[39m):\n\u001B[32m     30\u001B[39m         signal_array = group[\u001B[33m'\u001B[39m\u001B[33mmeasurement\u001B[39m\u001B[33m'\u001B[39m].values\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\EMG_project\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:618\u001B[39m, in \u001B[36mBaseGrouper.get_iterator\u001B[39m\u001B[34m(self, data, axis)\u001B[39m\n\u001B[32m    607\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mget_iterator\u001B[39m(\n\u001B[32m    608\u001B[39m     \u001B[38;5;28mself\u001B[39m, data: NDFrameT, axis: AxisInt = \u001B[32m0\u001B[39m\n\u001B[32m    609\u001B[39m ) -> Iterator[\u001B[38;5;28mtuple\u001B[39m[Hashable, NDFrameT]]:\n\u001B[32m    610\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    611\u001B[39m \u001B[33;03m    Groupby iterator\u001B[39;00m\n\u001B[32m    612\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m    616\u001B[39m \u001B[33;03m    for each group\u001B[39;00m\n\u001B[32m    617\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m618\u001B[39m     splitter = \u001B[38;5;28mself\u001B[39m._get_splitter(data, axis=axis)\n\u001B[32m    619\u001B[39m     keys = \u001B[38;5;28mself\u001B[39m.group_keys_seq\n\u001B[32m    620\u001B[39m     \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(keys, splitter)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\EMG_project\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:629\u001B[39m, in \u001B[36mBaseGrouper._get_splitter\u001B[39m\u001B[34m(self, data, axis)\u001B[39m\n\u001B[32m    622\u001B[39m \u001B[38;5;129m@final\u001B[39m\n\u001B[32m    623\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_get_splitter\u001B[39m(\u001B[38;5;28mself\u001B[39m, data: NDFrame, axis: AxisInt = \u001B[32m0\u001B[39m) -> DataSplitter:\n\u001B[32m    624\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    625\u001B[39m \u001B[33;03m    Returns\u001B[39;00m\n\u001B[32m    626\u001B[39m \u001B[33;03m    -------\u001B[39;00m\n\u001B[32m    627\u001B[39m \u001B[33;03m    Generator yielding subsetted objects\u001B[39;00m\n\u001B[32m    628\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m629\u001B[39m     ids, _, ngroups = \u001B[38;5;28mself\u001B[39m.group_info\n\u001B[32m    630\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m _get_splitter(\n\u001B[32m    631\u001B[39m         data,\n\u001B[32m    632\u001B[39m         ids,\n\u001B[32m   (...)\u001B[39m\u001B[32m    636\u001B[39m         axis=axis,\n\u001B[32m    637\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32mproperties.pyx:36\u001B[39m, in \u001B[36mpandas._libs.properties.CachedProperty.__get__\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\EMG_project\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:745\u001B[39m, in \u001B[36mBaseGrouper.group_info\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    743\u001B[39m \u001B[38;5;129m@cache_readonly\u001B[39m\n\u001B[32m    744\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mgroup_info\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> \u001B[38;5;28mtuple\u001B[39m[npt.NDArray[np.intp], npt.NDArray[np.intp], \u001B[38;5;28mint\u001B[39m]:\n\u001B[32m--> \u001B[39m\u001B[32m745\u001B[39m     comp_ids, obs_group_ids = \u001B[38;5;28mself\u001B[39m._get_compressed_codes()\n\u001B[32m    747\u001B[39m     ngroups = \u001B[38;5;28mlen\u001B[39m(obs_group_ids)\n\u001B[32m    748\u001B[39m     comp_ids = ensure_platform_int(comp_ids)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\EMG_project\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:769\u001B[39m, in \u001B[36mBaseGrouper._get_compressed_codes\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    766\u001B[39m     \u001B[38;5;66;03m# FIXME: compress_group_index's second return value is int64, not intp\u001B[39;00m\n\u001B[32m    768\u001B[39m ping = \u001B[38;5;28mself\u001B[39m.groupings[\u001B[32m0\u001B[39m]\n\u001B[32m--> \u001B[39m\u001B[32m769\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m ping.codes, np.arange(\u001B[38;5;28mlen\u001B[39m(ping._group_index), dtype=np.intp)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\EMG_project\\Lib\\site-packages\\pandas\\core\\groupby\\grouper.py:691\u001B[39m, in \u001B[36mGrouping.codes\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    689\u001B[39m \u001B[38;5;129m@property\u001B[39m\n\u001B[32m    690\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcodes\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> npt.NDArray[np.signedinteger]:\n\u001B[32m--> \u001B[39m\u001B[32m691\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._codes_and_uniques[\u001B[32m0\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32mproperties.pyx:36\u001B[39m, in \u001B[36mpandas._libs.properties.CachedProperty.__get__\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\EMG_project\\Lib\\site-packages\\pandas\\core\\groupby\\grouper.py:835\u001B[39m, in \u001B[36mGrouping._codes_and_uniques\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    830\u001B[39m     uniques = \u001B[38;5;28mself\u001B[39m._uniques\n\u001B[32m    831\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    832\u001B[39m     \u001B[38;5;66;03m# GH35667, replace dropna=False with use_na_sentinel=False\u001B[39;00m\n\u001B[32m    833\u001B[39m     \u001B[38;5;66;03m# error: Incompatible types in assignment (expression has type \"Union[\u001B[39;00m\n\u001B[32m    834\u001B[39m     \u001B[38;5;66;03m# ndarray[Any, Any], Index]\", variable has type \"Categorical\")\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m835\u001B[39m     codes, uniques = algorithms.factorize(  \u001B[38;5;66;03m# type: ignore[assignment]\u001B[39;00m\n\u001B[32m    836\u001B[39m         \u001B[38;5;28mself\u001B[39m.grouping_vector, sort=\u001B[38;5;28mself\u001B[39m._sort, use_na_sentinel=\u001B[38;5;28mself\u001B[39m._dropna\n\u001B[32m    837\u001B[39m     )\n\u001B[32m    838\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m codes, uniques\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\EMG_project\\Lib\\site-packages\\pandas\\core\\algorithms.py:795\u001B[39m, in \u001B[36mfactorize\u001B[39m\u001B[34m(values, sort, use_na_sentinel, size_hint)\u001B[39m\n\u001B[32m    792\u001B[39m             \u001B[38;5;66;03m# Don't modify (potentially user-provided) array\u001B[39;00m\n\u001B[32m    793\u001B[39m             values = np.where(null_mask, na_value, values)\n\u001B[32m--> \u001B[39m\u001B[32m795\u001B[39m     codes, uniques = factorize_array(\n\u001B[32m    796\u001B[39m         values,\n\u001B[32m    797\u001B[39m         use_na_sentinel=use_na_sentinel,\n\u001B[32m    798\u001B[39m         size_hint=size_hint,\n\u001B[32m    799\u001B[39m     )\n\u001B[32m    801\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m sort \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(uniques) > \u001B[32m0\u001B[39m:\n\u001B[32m    802\u001B[39m     uniques, codes = safe_sort(\n\u001B[32m    803\u001B[39m         uniques,\n\u001B[32m    804\u001B[39m         codes,\n\u001B[32m   (...)\u001B[39m\u001B[32m    807\u001B[39m         verify=\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[32m    808\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\EMG_project\\Lib\\site-packages\\pandas\\core\\algorithms.py:595\u001B[39m, in \u001B[36mfactorize_array\u001B[39m\u001B[34m(values, use_na_sentinel, size_hint, na_value, mask)\u001B[39m\n\u001B[32m    592\u001B[39m hash_klass, values = _get_hashtable_algo(values)\n\u001B[32m    594\u001B[39m table = hash_klass(size_hint \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(values))\n\u001B[32m--> \u001B[39m\u001B[32m595\u001B[39m uniques, codes = table.factorize(\n\u001B[32m    596\u001B[39m     values,\n\u001B[32m    597\u001B[39m     na_sentinel=-\u001B[32m1\u001B[39m,\n\u001B[32m    598\u001B[39m     na_value=na_value,\n\u001B[32m    599\u001B[39m     mask=mask,\n\u001B[32m    600\u001B[39m     ignore_na=use_na_sentinel,\n\u001B[32m    601\u001B[39m )\n\u001B[32m    603\u001B[39m \u001B[38;5;66;03m# re-cast e.g. i8->dt64/td64, uint8->bool\u001B[39;00m\n\u001B[32m    604\u001B[39m uniques = _reconstruct_data(uniques, original.dtype, original)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T10:04:49.285811Z",
     "start_time": "2025-05-13T10:04:49.137846Z"
    }
   },
   "cell_type": "code",
   "source": "import keras_tuner as kt",
   "id": "196c98fba052276e",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T10:06:04.968938Z",
     "start_time": "2025-05-13T10:06:04.956771Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LSTMHyperModel(kt.HyperModel):\n",
    "\n",
    "    def __init__(self, input_shape, num_classes):\n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    # ----------------- build -----------------------------------------\n",
    "    def build(self, hp):\n",
    "        lr = hp.Float('learning_rate', 1e-4, 5e-2, sampling='log')\n",
    "\n",
    "        opt  = hp.Choice('optimizer',\n",
    "                         values=['adam', 'rmsprop', 'nadam'])\n",
    "\n",
    "        norm = hp.Choice('normalization',\n",
    "                         values=['none', 'batch', 'layer'])\n",
    "\n",
    "        dropout           = hp.Float('dropout',           0.0, 0.5, step=0.1)\n",
    "        recurrent_dropout = hp.Float('recurrent_dropout', 0.0, 0.5, step=0.1)\n",
    "\n",
    "        act_dense = hp.Choice('act_dense', ['tanh', 'relu'])\n",
    "        act_lstm  = hp.Choice('act_lstm',  ['tanh', 'relu'])\n",
    "\n",
    "        # We only declare batch_size here; we'll use it in fit().\n",
    "        hp.Choice('batch_size', values=[32, 64, 128, 256, 512])\n",
    "\n",
    "        model = LSTM(\n",
    "            input_shape      = self.input_shape,\n",
    "            num_classes      = self.num_classes,\n",
    "            learning_rate    = lr,\n",
    "            optimizer        = opt,\n",
    "            normalization    = norm,\n",
    "            dropout          = dropout,\n",
    "            recurrent_dropout= recurrent_dropout,\n",
    "            act_dense        = act_dense,\n",
    "            act_lstm         = act_lstm\n",
    "        ).get_model()\n",
    "\n",
    "        return model\n",
    "\n",
    "    # ----------------- fit -------------------------------------------\n",
    "    def fit(self, hp, model, X_train, y_train, X_val, y_val, **kwargs):\n",
    "        \"\"\"\n",
    "        Called by the tuner for every trial.  We inject the per-trial\n",
    "        batch_size coming from hp.\n",
    "        \"\"\"\n",
    "        batch_size = hp.get('batch_size')\n",
    "        return model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            batch_size=batch_size,\n",
    "            epochs=kwargs.get('epochs', 10),\n",
    "            verbose=kwargs.get('verbose', 2)\n",
    "        )"
   ],
   "id": "c79576dba368e314",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T10:06:06.707458Z",
     "start_time": "2025-05-13T10:06:06.704295Z"
    }
   },
   "cell_type": "code",
   "source": [
    "stop_early = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_f1_score',      # metric name Keras assigns: ‘f1_score’\n",
    "    mode='max',                  # we want to maximise it\n",
    "    patience=5,\n",
    "    restore_best_weights=True)"
   ],
   "id": "3b49c0f097fba9f",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T10:06:09.752620Z",
     "start_time": "2025-05-13T10:06:09.748624Z"
    }
   },
   "cell_type": "code",
   "source": "hypermodel = LSTMHyperModel(input_shape, num_classes)",
   "id": "58ec55430db62dc4",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T10:06:10.083944Z",
     "start_time": "2025-05-13T10:06:10.075966Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.utils.path_utils import get_models_dir\n",
    "\n",
    "model_dir = get_models_dir() / \"LSTM_search\"\n",
    "model_dir"
   ],
   "id": "98f2f625c1de8688",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/Users/erik/IES_codebase/EMG_Project/CDT406-Smart-Gripper/models/LSTM_search')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T10:06:10.609140Z",
     "start_time": "2025-05-13T10:06:10.561109Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tuner = kt.BayesianOptimization(\n",
    "    hypermodel,\n",
    "    objective = kt.Objective(\"val_f1_score\", direction=\"max\"),\n",
    "    max_trials=50,\n",
    "    directory=model_dir,\n",
    "    project_name=\"baseline_v2\",\n",
    "    overwrite=True\n",
    ")"
   ],
   "id": "d526ca1e149121df",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tuner.search(X_train, y_train,\n",
    "             X_val=X_val, y_val=y_val,\n",
    "             callbacks=[stop_early],\n",
    "             epochs=30,\n",
    "             verbose=2)"
   ],
   "id": "c643dfb1ea10de35",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3 Complete [00h 00m 24s]\n",
      "val_f1_score: 0.48329830169677734\n",
      "\n",
      "Best val_f1_score So Far: 0.6024924516677856\n",
      "Total elapsed time: 00h 01m 36s\n"
     ]
    }
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T10:05:38.447936Z",
     "start_time": "2025-05-13T10:05:38.441421Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]"
   ],
   "id": "d0852abe3e47bf7c",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T10:09:44.126754Z",
     "start_time": "2025-05-13T10:09:44.118777Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Best hyper-parameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Corresponding trial (contains metrics)\n",
    "best_trial   = tuner.oracle.get_best_trials(num_trials=1)[0]\n",
    "best_val_f1  = best_trial.metrics.get_best_value('val_f1_score')\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete.\n",
    "\n",
    "Optimal learning rate     : {best_hps.get('learning_rate')}\n",
    "Optimal optimizer         : {best_hps.get('optimizer')}\n",
    "Optimal normalization     : {best_hps.get('normalization')}\n",
    "Optimal batch size        : {best_hps.get('batch_size')}\n",
    "Optimal dropout           : {best_hps.get('dropout')}\n",
    "Optimal recurrent dropout : {best_hps.get('recurrent_dropout')}\n",
    "Optimal dense activation  : {best_hps.get('act_dense')}\n",
    "Optimal LSTM activation   : {best_hps.get('act_lstm')}\n",
    "Best validation F1-score  : {best_val_f1:.4f}\n",
    "\"\"\")"
   ],
   "id": "6b7b54b5a9f6afdb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The hyperparameter search is complete.\n",
      "\n",
      "Optimal learning rate     : 0.0001009429310439601\n",
      "Optimal optimizer         : nadam\n",
      "Optimal normalization     : batch\n",
      "Optimal batch size        : 32\n",
      "Optimal dropout           : 0.2\n",
      "Optimal recurrent dropout : 0.4\n",
      "Optimal dense activation  : tanh\n",
      "Optimal LSTM activation   : relu\n",
      "Best validation F1-score  : 0.6025\n",
      "\n"
     ]
    }
   ],
   "execution_count": 24
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
